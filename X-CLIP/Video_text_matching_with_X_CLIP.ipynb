{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "321c2e40c6394ea3ad0d46cdcd265d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VideoModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VideoModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VideoView",
            "autoplay": true,
            "controls": true,
            "format": "mp4",
            "height": "",
            "layout": "IPY_MODEL_87d29fba2c714fe39e1f9787112e0448",
            "loop": true,
            "width": "500"
          }
        },
        "87d29fba2c714fe39e1f9787112e0448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wissemkarouss/CAN-secure_dream_drive/blob/main/X-CLIP/Video_text_matching_with_X_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up environment\n",
        "\n",
        "We'll first install ðŸ¤— Transformers (from Github as it's not yet included in a new release) and decord, which we'll use to decode a video."
      ],
      "metadata": {
        "id": "hSHCiig6MCtC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X6zy1c5L6sy",
        "outputId": "456989f6-bfc1-414e-f3ba-a35cb779b953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q decord"
      ],
      "metadata": {
        "id": "4_6cm80vMyy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load video\n",
        "\n",
        "Here we load a video of people eating spaghetti."
      ],
      "metadata": {
        "id": "z7Bcj5J1MEL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from ipywidgets import Video\n",
        "\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        ")\n",
        "Video.from_file(file_path, width=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "321c2e40c6394ea3ad0d46cdcd265d8f",
            "87d29fba2c714fe39e1f9787112e0448"
          ]
        },
        "id": "xExRsm5dM53A",
        "outputId": "e03852ac-b826-48bd-e391-4482be5c0b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\x0fI\\xb7mdat\\x00\\xâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "321c2e40c6394ea3ad0d46cdcd265d8f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll sample 8 frames from the video."
      ],
      "metadata": {
        "id": "LL8sKy_rNuCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from decord import VideoReader, cpu\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "file_path=\"/content/n area where abandoned items are being processed for recycling and recovery.mp4\"\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices\n",
        "\n",
        "vr = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n",
        "\n",
        "# sample 16 frames\n",
        "vr.seek(0)\n",
        "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=len(vr))\n",
        "video = vr.get_batch(indices).asnumpy()\n",
        "print(video.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is1p-s13MVyS",
        "outputId": "df52e979-b259-4235-b73c-5ef9c9ee35ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 1080, 1920, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference\n",
        "\n",
        "Finally, we forward the video + 3 possible texts through the X-CLIP model. The model will tell us how much each text matches with the given video."
      ],
      "metadata": {
        "id": "3qB8DlXHMV-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XCLIPProcessor, XCLIPModel\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/xclip-base-patch32-16-frames\"\n",
        "processor = XCLIPProcessor.from_pretrained(model_name)\n",
        "model = XCLIPModel.from_pretrained(model_name)\n",
        "\n",
        "inputs = processor(text=[\"playing sports\", \"n area where abandoned items are being processed for recycling and recovery\", \"go shopping\"], videos=list(video), return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "probs = outputs.logits_per_video.softmax(dim=1)\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZV0SpApMCUz",
        "outputId": "a3b604ba-1d91-462d-f058-57a8f479a462"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.5401e-04, 9.9871e-01, 9.3128e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import XCLIPProcessor, XCLIPModel\n",
        "from decord import VideoReader, cpu\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load model and processor\n",
        "model_name = \"microsoft/xclip-base-patch16-zero-shot\"\n",
        "processor = XCLIPProcessor.from_pretrained(model_name)\n",
        "model = XCLIPModel.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Function to load and sample video frames\n",
        "def load_video(video_path, num_frames=32, target_size=(224, 224)):\n",
        "    try:\n",
        "        vr = VideoReader(video_path, ctx=cpu(0))\n",
        "        total_frames = len(vr)\n",
        "        if total_frames < num_frames:\n",
        "            raise ValueError(f\"Video has only {total_frames} frames, but {num_frames} are required.\")\n",
        "\n",
        "        # Sample frames uniformly\n",
        "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "        frames = vr.get_batch(frame_indices).asnumpy()  # Shape: (num_frames, height, width, 3)\n",
        "\n",
        "        # Resize frames to target_size (224, 224) for X-CLIP\n",
        "        resized_frames = []\n",
        "        for frame in frames:\n",
        "            # Convert to PIL Image for resizing\n",
        "            frame = Image.fromarray(frame)\n",
        "            frame = frame.resize(target_size, Image.Resampling.LANCZOS)\n",
        "            resized_frames.append(np.array(frame))\n",
        "\n",
        "        frames = np.array(resized_frames, dtype=np.uint8)  # Shape: (num_frames, 224, 224, 3)\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading video: {e}\")\n",
        "        return None # Return None to indicate failure\n",
        "\n",
        "# Load video (replace with your video path)\n",
        "video_path = \"/content/cooking.mp4\"  # Replace with your actual video file path\n",
        "video_frames = load_video(video_path)\n",
        "\n",
        "if video_frames is not None:\n",
        "    print(f\"Loaded frames shape: {video_frames.shape}\")  # Should be (32, 224, 224, 3)\n",
        "\n",
        "    # Text descriptions: positive and negative prompts\n",
        "    positive_description = \"cat walking\"  # Replace with your description\n",
        "    negative_description = \"irrelavant\"  # Negative (irrelevant) prompt\n",
        "    text_descriptions = [positive_description, negative_description]\n",
        "\n",
        "    # Process video and text\n",
        "    try:\n",
        "        inputs = processor(\n",
        "            text=text_descriptions,\n",
        "            videos=list(video_frames),  # Pass the loaded frames as a list\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing inputs: {e}\")\n",
        "        inputs = None # Set inputs to None to indicate failure\n",
        "\n",
        "    if inputs is not None: # Corrected syntax here\n",
        "        # Move inputs to the same device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Run inference\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits_per_video  # Shape: (1, 2) for two descriptions\n",
        "                probs = logits.softmax(dim=1)  # Convert logits to probabilities\n",
        "                positive_prob = probs[0, 0].item()  # Probability for positive description\n",
        "                negative_prob = probs[0, 1].item()  # Probability for negative description\n",
        "        except Exception as e:\n",
        "            print(f\"Error during inference: {e}\")\n",
        "            positive_prob = negative_prob = None\n",
        "\n",
        "        if positive_prob is not None:\n",
        "            # Interpret relevance\n",
        "            threshold = 0.5  # Probability threshold (adjust as needed)\n",
        "            is_relevant = positive_prob > threshold\n",
        "            print(f\"Positive Description: '{positive_description}' | Probability Score: {positive_prob:.4f}\")\n",
        "            print(f\"Negative Description: '{negative_description}' | Probability Score: {negative_prob:.4f}\")\n",
        "            print(f\"Is the positive description relevant? {'Yes' if is_relevant else 'No'}\")\n",
        "        else:\n",
        "            print(\"Inference failed.\")\n",
        "    else:\n",
        "        print(\"Input processing failed.\")\n",
        "else:\n",
        "    print(\"Video loading failed, skipping inference.\")"
      ],
      "metadata": {
        "id": "Ed3qWm15N4dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92b087e-1b69-46ed-dd87-71d35d88e7ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded frames shape: (32, 224, 224, 3)\n",
            "Positive Description: 'cat walking' | Probability Score: 0.4141\n",
            "Negative Description: 'irrelavant' | Probability Score: 0.5859\n",
            "Is the positive description relevant? No\n"
          ]
        }
      ]
    }
  ]
}